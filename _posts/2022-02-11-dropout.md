---
title: Is Dropout actually useful?
author: Mustafa Khan
date: 2022-02-1
category: Jekyll
layout: post
---

Dropout is a regularization method introduced in this [paper](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) intended to avoid overfitting.
It approximates training a large number of neural networks with different architectures in parallel.

During training, some number of layer outputs are randomly ignored or “dropped out.” 
This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. 
In effect, each update to a layer during training is performed with a different “view” of the configured layer.

The question I'm interested in is does this work in theory and in practice? What are some heuristics or takeaways for uding dropout in training a NN and how can overfitting be reduced?

## Sources
* https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/
* https://www.machinecurve.com/index.php/2019/12/16/what-is-dropout-reduce-overfitting-in-your-neural-networks/
